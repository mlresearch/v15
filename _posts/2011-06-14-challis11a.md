---
title: Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian
  Linear Models
abstract: Two popular approaches to forming bounds in approximate Bayesian inference
  are local variational methods and minimal Kullback-Leibler divergence methods. For
  a large class of models we explicitly relate the two approaches, showing that the
  local variational method is equivalent to a weakened form of Kullback-Leibler Gaussian
  approximation.  This gives a strong motivation to develop efficient methods for  KL
  minimisation.  An important and previously unproven property of the KL variational
  Gaussian bound is that it is a concave function in the parameters of the Gaussian
  for log concave sites. This observation, along with compact concave parametrisations
  of the covariance, enables us to develop fast scalable optimisation procedures to
  obtain lower bounds on the marginal likelihood in large scale Bayesian linear models.
pdf: http://proceedings.mlr.press/v15/challis11a/challis11a.pdf
supplementary: http://proceedings.mlr.press/v15/challis11a/challis11aSupple.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: challis11a
month: 0
tex_title: Concave Gaussian Variational Approximations for Inference in Large-Scale
  Bayesian Linear Models
firstpage: 199
lastpage: 207
page: 199-207
order: 199
cycles: false
author:
- given: Edward
  family: Challis
- given: David
  family: Barber
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
