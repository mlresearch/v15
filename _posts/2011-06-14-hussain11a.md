---
title: Improved Loss Bounds For Multiple Kernel Learning
abstract: We propose two new generalization error bounds for multiple kernel learning
  (MKL). First, using the bound of Srebro and Ben-David (2006) as a starting point,
  we derive a new version which uses a simple counting argument for the choice of
  kernels in order to generate a tighter bound when 1-norm regularization (sparsity)
  is imposed in the kernel learning problem. The second bound is a Rademacher complexity
  bound which is additive in the (logarithmic) kernel complexity and margin term.
  This dependency is superior to all previously published Rademacher bounds for learning
  a convex combination of kernels, including the recent bound of Cortes et al. (2010),
  which exhibits a multiplicative interaction. We illustrate the tightness of our
  bounds with simulations. [pdf] [errata]
pdf: http://proceedings.mlr.press/v15/hussain11a/hussain11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hussain11a
month: 0
firstpage: 370
lastpage: 377
page: 370-377
sections: 
author:
- given: Zakria
  family: Hussain
- given: John
  family: Shaweâ€“Taylor
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
