---
title: Relative Entropy Inverse Reinforcement Learning
abstract: We consider the problem of imitation learning where the examples, demonstrated
  by an expert, cover only a small part of a large state space. Inverse Reinforcement
  Learning (IRL) provides an efficient tool for generalizing the demonstration, based
  on the assumption that the expert is optimally acting in a Markov Decision Process
  (MDP). Most of the past work on IRL requires that a (near)-optimal policy can be
  computed for different reward functions. However, this requirement can hardly be
  satisfied in systems with a large, or continuous, state space. In this paper, we
  propose a model-free IRL algorithm, where the relative entropy between the empirical
  distribution of the state-action trajectories under a baseline policy and their
  distribution under the learned policy is minimized by stochastic gradient descent.
  We compare this new approach to well-known IRL algorithms using learned MDP models.
  Empirical results on simulated car racing, gridworld and ball-in-a-cup problems
  show that our approach is able to learn good policies from a small number of demonstrations.
pdf: http://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: boularias11a
month: 0
tex_title: Relative Entropy Inverse Reinforcement Learning
firstpage: 182
lastpage: 189
page: 182-189
order: 182
cycles: false
author:
- given: Abdeslam
  family: Boularias
- given: Jens
  family: Kober
- given: Jan
  family: Peters
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
