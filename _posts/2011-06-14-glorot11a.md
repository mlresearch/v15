---
title: Deep Sparse Rectifier Neural Networks
abstract: While logistic sigmoid neurons are more biologically plausible than hyperbolic
  tangent neurons, the latter work better for training multi-layer neural networks.
  This paper shows that rectifying neurons are an even better model of biological
  neurons and yield equal or better performance than hyperbolic tangent networks in
  spite of the hard non-linearity and non-differentiability at zero, creating sparse
  representations with true zeros which seem remarkably suitable for naturally sparse
  data. Even though they can take advantage of semi-supervised setups with extra-unlabeled
  data, deep rectifier networks can reach their best performance without requiring
  any unsupervised pre-training on purely supervised tasks with large labeled datasets.
  Hence, these results can be seen as a new milestone in the attempts at understanding
  the difficulty in training deep but purely supervised neural networks, and closing
  the performance gap between neural networks learnt with and without unsupervised
  pre-training.
pdf: http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: glorot11a
month: 0
tex_title: Deep Sparse Rectifier Neural Networks
firstpage: 315
lastpage: 323
page: 315-323
order: 315
cycles: false
author:
- given: Xavier
  family: Glorot
- given: Antoine
  family: Bordes
- given: Yoshua
  family: Bengio
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
