---
section: notable
title: Learning Scale Free Networks by Reweighted $\ell_1$ regularization
abstract: Methods for $\ell_1$-type regularization have been widely used in Gaussian graphical
  model selection tasks to encourage sparse structures. However, often we would like
  to include more structural information than mere sparsity. In this work, we focus
  on learning so-called “scale-free” models,  a common feature that appears in many
  real-work networks. We replace the $\ell_1$ regularization with a power law regularization
  and  optimize the objective function by a sequence of iteratively reweighted $\ell_1$
  regularization problems, where the regularization coefficients of nodes with high
  degree are reduced, encouraging the appearance of hubs with high degree. Our method
  can be easily adapted to improve any existing $\ell_1$-based methods, such as graphical
  lasso, neighborhood selection, and JSRM when the underlying networks are believed
  to be scale free or have dominating hubs. We demonstrate in simulation that our
  method significantly outperforms the a baseline $\ell_1$ method at learning scale-free
  networks and hub networks, and also illustrate its behavior on gene expression data.
pdf: http://proceedings.mlr.press/v15/liu11a/liu11a.pdf
discussion: agarwal11a.html
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liu11a
month: 0
tex_title: Learning Scale Free Networks by Reweighted $\ell_1$ regularization
firstpage: 40
lastpage: 48
page: 40-48
order: 40
cycles: false
author:
- given: Qiang
  family: Liu
- given: Alexander
  family: Ihler
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
