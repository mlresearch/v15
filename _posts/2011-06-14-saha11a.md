---
title: Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit
  Feedback
abstract: The study of online convex optimization in the bandit setting was initiated
  by Kleinberg (2004) and Flaxman et al. (2005). Such a setting models a decision
  maker that has to make decisions in the face of adversarially chosen convex loss
  functions. Moreover, the only information the decision maker receives are the losses.
  The identity of the loss functions themselves is not revealed. In this setting,
  we reduce the gap between the best known lower and upper bounds for the class of
  smooth convex functions, i.e. convex functions with a Lipschitz continuous gradient.
  Building upon existing work on self-concordant regularizers and one-point gradient
  estimation, we give the first algorithm whose expected regret, ignoring constant
  and logarithmic factors, is $O(T^{2/3})$.
pdf: http://proceedings.mlr.press/v15/saha11a/saha11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: saha11a
month: 0
tex_title: Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit
  Feedback
firstpage: 636
lastpage: 642
page: 636-642
order: 636
cycles: false
author:
- given: Ankan
  family: Saha
- given: Ambuj
  family: Tewari
date: 2011-06-14
address: Fort Lauderdale, FL, USA
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
